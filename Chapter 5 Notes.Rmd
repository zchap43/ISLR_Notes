---
title: "Chapter 5 Notes"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Previous chapters we need to know for this one:
##Estimates:
$$ \hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}$$

$$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}$$
where $\bar{x}$ and $\bar{y}$ are the sample means
##Standard Error Estimates
$$ SE(\hat{\beta_0})^2 = \sigma^2\bigg[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i = 1}^n(x_i - x)^2}\bigg]$$
$$ SE(\hat{\beta_1})^2 = \sigma^2\bigg[\frac{1}{n} + \frac{\sigma^2}{\sum_{i = 1}^n(x_i - x)^2}\bigg]$$
##Variance of sample mean
$$Var(\hat{\mu}) = SE(\hat{\mu})^2 = \frac{\sigma^2}{n}$$

#Beyes error Rate formula. Pretty important

$$ p = 1 - \sum_{C_i \neq C_{max,x}}\int_{x \epsilon H_i} P(C_i|x)p(x)dx$$

Estimate linear regression variability by
pulling multiple samples and observing variance

2 Methods of focus: Cross validation and Bootsrap

Cross Validation: reviews the test error of a model

Bootstrap: provides a measure of accuracy of a 
parameter estimate or model

Model assessment: Evaluating model performace

Model Selection: Choosing model with proper
flexibility

#Cross Validation

##Validation Set Approach

Validation set approach(VSA): randomly divide the data

###Drawbacks of VSA

MSE can be highly variable depending on data split

Only the data in training set is used to fit the
model. This is an issue especially in parameterized models

##Leave one out cross validation

Close to VSA, but refined to fix drawbacks

Remove one "row" from the dataset, then fit the model (n - 1 training set, 1 testing set)

MSE of the testing set is unbiased

formula for LOOCV: $$CV_{(n)} = \frac{1}{n}\sum_{i=1}^n MSE_i$$

###Advantages

far less bias than VSA

Does not overestimate the test error rate

Results of LOOCV on same dataset will not vary

like VSA

Works with most or all models

Finds minimum of Test MSE (but not the value)

###Disadvantages

Expensive to implement(n model fits per model)

###Shortcut LOOCV

This only works with regression models
$$cv_{(n)} = \frac{1}{n}\sum_i^n (\frac{y - \hat{y_i}}{1- h_i})^2$$

##K-Fold CV

Divide dataset into k groups then select one group
as test group and train the model

formula:
$$CV_{(k)} = \frac{1}{k}\sum_{i=1}^k MSE_i$$

###Advantages

Less computationally expensive compared to LOOCV

applicable to any statistical modeling practice

More control of bias-variance tradeoff

Lower variance than VSA

Finds minimum of Test MSE (but not the value)

Also, gets closer to test error rate than LOOCV

Lower Variance than LOOCV

###Disadvantages

Higher Bias than LOOCV

##Cross validation on Classification Problems

Instead of MSE, we will use a count of misclassified tests

$$ CV_{(n)} = \frac{1}{n}\sum_{i=1}^n Err_i $$
We cannot calculate the beyes error rate on a
real world dataset, so finding a model with the
proper flexibility will be an issue.

Luckily, we can use the LOOCV or K-fold to find
the right flexibility to minimize the error rate

#BootStrapOn

##Introduction

Can estimate the standard errors of the coefficients from a variety of models
This is good beacause some models do not have
easily calculatable variance

formula:
X and Y - random variables

$$\alpha $$- percentage allocated to one variable

$$1 - \alpha  $$- percentage allocated to the other variable

$$\sigma_X^2 = Var(X) $$

$$\sigma_y^2 = Var(Y)$$

$$\sigma_{XY} = Cov(X,y)$$
For finding:
$$ Var(\alphaX + (1-\alpha)Y)$$
We can use
$$ \alpha = \frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}}$$

Because we do not know the variance or covariance, we will use previous X and Y values to estimate variance and covariance.

then, we can estimate the variance with:
$$ \hat{\alpha} = \frac{\hat{\sigma}_y^2 - \hat{\sigma}_{xy}^2}{\hat{\sigma_x^2} + \hat{\sigma}_y^2 - 2\hat{\sigma}_{xy}^2}$$

to estimate $\hat{\sigma}$ we can take multiple samples from the original dataset.
These samples can contain each row only once, but one row can be included in multiple sample sets

we then continue to create new alpha estimates by continuously creating new data sets. this then allows us to make a better estimation of alpha with the given dataset. we will plug all these alphas into the equation:

$$ SE_B(\hat{\alpha})=\sqrt{\frac{1}{B-1}\sum_{r=1}^B(\hat{\alpha}^{*r} - \frac{1}{B}\sum_{r`=1}^B \hat{\alpha}^{*r`})}$$

#Lab Chapter 5

```{r}
# The Validation Set Approach

library(ISLR)
set.seed(1)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
set.seed(2)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
```
```{r}
# Leave-One-Out Cross-Validation

glm.fit=glm(mpg~horsepower,data=Auto)
coef(glm.fit)
lm.fit=lm(mpg~horsepower,data=Auto)
coef(lm.fit)
library(boot)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.ersr=cv.glm(Auto,glm.fit)
cv.err$delta
cv.error=rep(0,5)
for (i in 1:5){
 glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
 cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
 }
cv.error
```
```{r}
# k-Fold Cross-Validation

set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10){
 glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
 cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
 }
cv.error.10
```
```{r}
# The Bootstrap

alpha.fn=function(data,index){
 #takes x and y then returns alpha estimate
  X=data$X[index]
 Y=data$Y[index]
 ## formula for est. alpha
 return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
}
#Runs Portfolio through once
alpha.fn(Portfolio,1:100)
set.seed(1)
## Runs portfolio through a random sample once
alpha.fn(Portfolio,sample(100,100,replace=T))
## Runs alpha function on portfolio 1000 times and 
## Returns results
boot(Portfolio,alpha.fn,R=1000)

# Estimating the Accuracy of a Linear Regression Model

##Idea is to test our current model against multiple samples
##Then we can look at how it performs on multiple samples
##Finally allowing us to see if model is god

## Creating function to return alpha estimate of model
boot.fn=function(data,index)
 return(coef(lm(mpg~horsepower,data=data,subset=index)))

##Running boot on the dataset and returning coefficients
boot.fn(Auto,1:392)
set.seed(1)
## running on dataset 392 times in different orders(with replacement)
boot.fn(Auto,sample(392,392,replace=T))
##Still the same as the previous line, just to show the change in model accuracy
boot.fn(Auto,sample(392,392,replace=T))
## Samples 1000 random sets and computes standard errors
## Then use SE estimate to find a good bias and intercept
boot(Auto,boot.fn,1000)

##Shows the linear model fitment and standard error formulas from chapter 3. Notice the changes in standard error
##Difference in model estimate is that we do not consider noise to be the cause of variation in the data
summary(lm(mpg~horsepower,data=Auto))$coef 


boot.fn=function(data,index)

coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,subset=index))

set.seed(1)



boot(Auto,boot.fn,1000)


summary(lm(mpg~horsepower+I(horsepower^2),data=Auto))$coef
```

#EOC Problems



